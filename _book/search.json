[{"path":"index.html","id":"welcome","chapter":"1 Welcome!","heading":"1 Welcome!","text":"Hi! welcome, portfolio. collection different skills posses way show can data analysis context. page contains different group skills stands alone presentation. Also good know course, ongoing process.","code":""},{"path":"index.html","id":"about-me","chapter":"1 Welcome!","heading":"1.1 About Me","text":"student university applied sciences Utrecht. great passion biology data analysis. work enviorment can implement interests dream job . biology background data analysis knowledge can bridge lab analysts data scientists. transgender nonbinary individual, means go /(die/diens hen/hun). Usually isn’t something place much importance , just live life like anyone else might implications future regarding blue surgeries. don’t expect happen following year just case feel like future employer informed .","code":""},{"path":"example-analysis-c.-elegans-experiment.html","id":"example-analysis-c.-elegans-experiment","chapter":"2 Example analysis C. Elegans experiment","heading":"2 Example analysis C. Elegans experiment","text":"First look case analysis C. elegans plate experiment. data experiment kindly supplied J. Louter (INT/ILC). exposure various different compounds certain incubation time C. elegans nematode offspring counted determine toxicity compounds. data supplied excel file Data Link.looking data number things popped . compound unit values aren’t . values controls listed ‘pct’, plot graph see comparison nematode offspring treated control groups. wil allow us make guess efficacy compounds question. S-medium Ethanol controls study. Ethanol concentration 1.5 pct positive control, Medium-S negative control concentration 0.0 pct lastly Ethanol concentration 0.5 pct control vehicle . better illustrate position values graph set amount jitter added, 0.5 width none height, just pull apart points better visualization. y-axis values remain thus unchanged seeing concentration values within groups disturb data.Looking graph can estimate might correlation less offspring counted trated colonies C. Elegans higher concentrations Naphtalene, 2,6-diisopropylnaphthalene, decane Ethanol. continue test assumption data normalized plotted using min-max scaling. allow us better look different compounds compared .Based last graph able say might correlation higher dosage different compounds (controls, except positive control) reduced offspring different C. elegans populations counted.","code":""},{"path":"open-peer-review.html","id":"open-peer-review","chapter":"3 Open Peer Review","heading":"3 Open Peer Review","text":"context reproducibility wise ensure quality products kept high analyzing reviewing peoples work. due time recognizing bad transparency become easier ones work improve.","code":""},{"path":"open-peer-review.html","id":"teaching-anxiety-stress-and-resilience-during-the-covid-19-pandemic","chapter":"3 Open Peer Review","heading":"3.0.1 Teaching anxiety, stress and resilience during the COVID-19 pandemic","text":"first article selected reviewing teaching anxiety, stress resilience COVID-19 pandemic (Delgado-Gallegos et al. 2021).article looks psychological changes among teachers changed school attending curriculum virtual classroom model following restrictions face face meeting due pandemic quarantine.start scoring article based rubric determines reproducibility allow quick overview.Reviewing rubric shows us article professional overall. data supplied allows later testing reproducibility. Although actually check reproducibility need write code run analysis check variances results, can’t fully conclude article question fully reproducible. code supplied article beautiful example reproducible research.","code":""},{"path":"open-peer-review.html","id":"monitoring-trends-and-differences-in-covid-19-case-fatality-rates-using-decomposition-methods-contributions-of-age-structure-and-age-specific-fatality","chapter":"3 Open Peer Review","heading":"3.0.2 Monitoring trends and differences in COVID-19 case-fatality rates using decomposition methods: Contributions of age structure and age-specific fatality","text":"Now going look code article qualify reproducible research. next paper Monitoring trends differences COVID-19 case-fatality rates using decomposition methods: Contributions age structure age-specific fatality (Dudel et al., n.d.).First lets score article question using rubric :sets article apart previous one possibility look code used produce results raw data. raw data links exact code supplied article thus allow easy reproduction research done. important note however article doesn’t score perfectly rubric seen .code found R file name distributed based function code different scripts. scripts easy read clear function. Every section well defined borders short clear comments purpose chunk . rate code 4/5 first inspection. reason don’t give full score personal preference. prefer different scripts simply one script easy run instead reloading data change every time, ’s just .first script (00_functions.R) defines custom functions: one summarises data another function compare case fatality rates age distribution two groups.\nsecond code file (01_input.R) loads raw data, adds column (see mutate() function used), filters transfers new .csv file later usage.\nthird script (02_analysis.R) modifies data bit runs basic analysis .won’t go detail scripts analysis research purpose open peer review fragment. however try reproduce small portion following segment. changes code recorded comments.looking code plotting first figure can conclude following. code supplied beautifully made small change file location runs perfectly. one means reproducible research data.\nscore code research paper 5 5 ease use exactly supposed without noise .","code":"\n### Packages ##################################################################\n\n  library(tidyverse)\n  library(ggrepel)\n  library(scales)\n\n### Load data #################################################################\n\n  # Load data\n  db_gh <- read_csv(here::here(\"./data/inputdata.csv\"))\n  ##### Small change to the location of the file so i can run it properly\n\n### Aggregate data ############################################################\n\n  # Filter date\n  db_gh$Date <- as.Date(db_gh$Date,\"%d.%m.%y\")\n  db_gh2 <- db_gh %>% filter(Date<=as.Date(\"30.06.2020\",\"%d.%m.%y\"))\n  \n  # Set New York as \"country\" (easier handling)\n  db_gh2$Country[db_gh2$Country==\"USA\" & db_gh2$Region == \"NYC\"] <- \"NYC\"\n  \n  # Sum data over age groups\n  db_gh2 <- db_gh2 %>% \n    filter(!Country %in% c(\"China\",\"USA\",\"South Korea\") & Sex == \"b\") %>% \n    group_by(Country, Code,Date) %>% \n    summarise(Cases = sum(Cases),\n              Deaths = sum(Deaths))\n\n  # Exclude bolletino \n  db_gh2 <- db_gh2 %>%\n    filter(str_sub(Code, 1, 5) != \"ITbol\")\n  \n  # Sort by date\n  db_gh2 <- db_gh2 %>% group_by(Country) %>% arrange(Date)\n  \n  # Smooth reporting issues cases\n  for(country in unique(db_gh2$Country)) {\n    \n    days <- db_gh2$Date[db_gh2$Country==country]\n    \n    for(day in 2:length(days)) {\n      current <- db_gh2$Cases[db_gh2$Country==country & db_gh2$Date==days[day]]\n      previous <- db_gh2$Cases[db_gh2$Country==country & db_gh2$Date==days[day-1]]\n      \n      if(current<previous) db_gh2$Cases[db_gh2$Country==country & db_gh2$Date==days[day]] <- previous\n      \n    }\n    \n  }\n\n  # Smooth reporting issues deaths\n  for(country in unique(db_gh2$Country)) {\n    \n    days <- db_gh2$Date[db_gh2$Country==country]\n    \n    for(day in 2:length(days)) {\n      current <- db_gh2$Deaths[db_gh2$Country==country & db_gh2$Date==days[day]]\n      previous <- db_gh2$Deaths[db_gh2$Country==country & db_gh2$Date==days[day-1]]\n      \n      if(current<previous) db_gh2$Deaths[db_gh2$Country==country & db_gh2$Date==days[day]] <- previous\n      \n    }\n    \n  }\n  \n### Plot settings #############################################################\n\n  # Set colors\n  col_country <- c(\"Germany\" = \"black\",\n                   \"Italy\" = \"#2ca25f\",\n                   \"NYC\"=\"#f0027f\",\n                   \"Spain\"=\"#beaed4\",\n                   \"South Korea\"=\"#fdc086\")#,\n                   #\"USA\"=\"#386cb0\")\n  \n  cols <- c(\"black\",\n            \"#2ca25f\",\n            \"#f0027f\",\n            \"#beaed4\",\n            \"#fdc086\")#,\n            #\"#386cb0\")\n  \n  \n  # Axis\n  labs <- db_gh2 %>%\n    group_by(Country) %>% \n    filter(Cases == max(Cases)) %>% \n    mutate(Cases = Cases + 3000)\n  \n  # Including all reports\n  tx <- 6\n  lim_x <- 240000\n\n### Plot ######################################################################\n\n  db_gh2 %>% \n    ggplot(aes(Cases, Deaths, col = Country))+\n    geom_line(size = 1, alpha = .9)+\n    scale_x_continuous(expand = c(0,0), breaks = seq(0, 300000, 50000), limits = c(0, lim_x + 30000), labels = comma)+\n    scale_y_continuous(expand = c(0,0), breaks = seq(0, 40000, 5000), limits = c(0, 40000), labels = comma)+\n    annotate(\"segment\", x = 0, y = 0, xend = lim_x, yend = lim_x * .02, colour = \"grey40\", size = .5, alpha = .3, linetype = 2)+\n    annotate(\"segment\", x = 0, y = 0, xend = lim_x, yend = lim_x * .05, colour = \"grey40\", size = .5, alpha = .3, linetype = 2)+\n    annotate(\"segment\", x = 0, y = 0, xend = lim_x, yend = lim_x * .10, colour = \"grey40\", size = .5, alpha = .3, linetype = 2)+\n    annotate(\"segment\", x = 0, y = 0, xend = lim_x, yend = lim_x * .15, colour = \"grey40\", size = .5, alpha = .3, linetype = 2)+\n    annotate(\"text\", label = \"2% CFR\", x = lim_x + 1000, y = lim_x * .02,\n             color=\"grey30\", size = tx * .3, alpha = .6, hjust = 0, lineheight = .8) +\n    annotate(\"text\", label = \"5% CFR\", x = lim_x + 1000, y = lim_x * .05,\n             color=\"grey30\", size = tx * .3, alpha = .6, hjust = 0, lineheight = .8) +\n    annotate(\"text\", label = \"10% CFR\", x = lim_x + 1000, y = lim_x * .10,\n             color=\"grey30\", size = tx * .3, alpha = .6, hjust = 0, lineheight = .8) +\n    annotate(\"text\", label = \"15% CFR\", x = lim_x + 1000, y = lim_x * .15,\n             color=\"grey30\", size = tx * .3, alpha = .6, hjust = 0, lineheight = .8) +\n    scale_colour_manual(values = cols)+\n    geom_text(data = labs, aes(Cases, Deaths, label = Country),\n              size = tx * .35, hjust = 0, fontface = \"bold\") +\n    theme_classic()+\n    labs(x = \"Cases\", \n         y = \"Deaths\")+\n    theme(\n      panel.grid.minor = element_blank(),\n      legend.position = \"none\",\n      plot.margin = margin(5,5,5,5,\"mm\"),\n      axis.text.x = element_text(size = tx),\n      axis.text.y = element_text(size = tx),\n      axis.title.x = element_text(size = tx + 1),\n      axis.title.y = element_text(size = tx + 1)\n    )"},{"path":"guerrilla-analytics.html","id":"guerrilla-analytics","chapter":"4 Guerrilla Analytics","heading":"4 Guerrilla Analytics","text":"working together groups using github alone ones silico interface great importance keep consistent organized structure easily find data code. one hand crucial avoid confusion version usage (v4_file instead latestlateslatest_file) established files data named README.txt file explanation METADATA file. hand makes writing code edit, analyse plot data much easier path data files fixed. Guerrilla analytics reproducibility allows anyone quickly understand files data contain helps analysts write easier code deepening file locations.short example set type shown . organized one courses guerrilla analytics format better explain structure. easy work much enjoy looking well organized project.Although course organized properly allot missing items like missing code chunks due running code changes data Rmarkdown instead individual scripts. Note also consists 3 projects, just one. means depth comparison single project, said , depth creates chaos. run trough linux server. next example can see structure project, portfolio project (bookdown project portfolio witch linked trough website!). organized structure format pc. allows easier access compared server designed learn run learning institution.\npath shown longer necessary working server github project.Next looking README Portfolio project. Listed content README file show information files project. can also just look README linked Github link. one little bit easier read.","code":"# Using this command we can get the structure of our file organisation \nfs::dir_tree(here::here(\"daur2/\"))\n\n/home/1686589/daur2/\n├── rna_seq_ipsc\n│   ├── README.txt\n│   ├── code\n│   │   └── Input.R\n│   ├── data\n│   ├── rnaseq_ipsc.Rmd\n│   └── rnaseq_ipsc.html\n├── rnaseq_airway\n│   ├── README.txt\n│   ├── code\n│   │   ├── input.R\n│   │   └── namen.txt\n│   ├── data\n│   │   ├── airway_sampledata.csv\n│   │   ├── bam\n│   │   │   ├── SRR1039508.bam\n│   │   │   ├── SRR1039508.bam.indel.vcf\n│   │   │   ├── SRR1039508.bam.summary\n│   │   │   ├── SRR1039509.bam\n│   │   │   ├── SRR1039509.bam.indel.vcf\n│   │   │   ├── SRR1039509.bam.summary\n│   │   │   ├── SRR1039512.bam\n│   │   │   ├── SRR1039512.bam.indel.vcf\n│   │   │   ├── SRR1039512.bam.summary\n│   │   │   ├── SRR1039513.bam\n│   │   │   ├── SRR1039513.bam.indel.vcf\n│   │   │   ├── SRR1039513.bam.summary\n│   │   │   ├── SRR1039516.bam\n│   │   │   ├── SRR1039516.bam.indel.vcf\n│   │   │   ├── SRR1039516.bam.summary\n│   │   │   ├── SRR1039517.bam\n│   │   │   ├── SRR1039517.bam.indel.vcf\n│   │   │   ├── SRR1039517.bam.summary\n│   │   │   ├── SRR1039520.bam\n│   │   │   ├── SRR1039520.bam.indel.vcf\n│   │   │   ├── SRR1039520.bam.summary\n│   │   │   ├── SRR1039521.bam\n│   │   │   ├── SRR1039521.bam.indel.vcf\n│   │   │   ├── SRR1039521.bam.summary\n│   │   │   └── alignment_statistics.rds\n│   │   ├── counts\n│   │   │   └── read_counts.rds\n│   │   ├── fastq\n│   │   │   ├── SRR1039508_1.fastq.gz\n│   │   │   ├── SRR1039508_2.fastq.gz\n│   │   │   ├── SRR1039509_1.fastq.gz\n│   │   │   ├── SRR1039509_2.fastq.gz\n│   │   │   ├── SRR1039512_1.fastq.gz\n│   │   │   ├── SRR1039512_2.fastq.gz\n│   │   │   ├── SRR1039513.fastq.gz\n│   │   │   ├── SRR1039513_1.fastq.gz\n│   │   │   ├── SRR1039513_2.fastq.gz\n│   │   │   ├── SRR1039516.fastq.gz\n│   │   │   ├── SRR1039516_1.fastq.gz\n│   │   │   ├── SRR1039516_2.fastq.gz\n│   │   │   ├── SRR1039517_1.fastq.gz\n│   │   │   ├── SRR1039517_2.fastq.gz\n│   │   │   ├── SRR1039520.fastq.gz\n│   │   │   ├── SRR1039520_1.fastq.gz\n│   │   │   ├── SRR1039520_2.fastq.gz\n│   │   │   ├── SRR1039521.fastq.gz\n│   │   │   ├── SRR1039521_1.fastq.gz\n│   │   │   ├── SRR1039521_2.fastq.gz\n│   │   │   ├── fastq_dump_downloader.sh\n│   │   │   ├── fastqc.sh\n│   │   │   └── refgenome_downloader.sh\n│   │   ├── fastqc_output\n│   │   │   ├── SRR1039508_1_fastqc.html\n│   │   │   ├── SRR1039508_1_fastqc.zip\n│   │   │   ├── SRR1039508_2_fastqc.html\n│   │   │   ├── SRR1039508_2_fastqc.zip\n│   │   │   ├── SRR1039509_1_fastqc.html\n│   │   │   ├── SRR1039509_1_fastqc.zip\n│   │   │   ├── SRR1039509_2_fastqc.html\n│   │   │   ├── SRR1039509_2_fastqc.zip\n│   │   │   ├── SRR1039512_1_fastqc.html\n│   │   │   ├── SRR1039512_1_fastqc.zip\n│   │   │   ├── SRR1039512_2_fastqc.html\n│   │   │   ├── SRR1039512_2_fastqc.zip\n│   │   │   ├── SRR1039513_1_fastqc.html\n│   │   │   ├── SRR1039513_1_fastqc.zip\n│   │   │   ├── SRR1039513_2_fastqc.html\n│   │   │   ├── SRR1039513_2_fastqc.zip\n│   │   │   ├── SRR1039513_fastqc.html\n│   │   │   ├── SRR1039513_fastqc.zip\n│   │   │   ├── SRR1039516_1_fastqc.html\n│   │   │   ├── SRR1039516_1_fastqc.zip\n│   │   │   ├── SRR1039516_2_fastqc.html\n│   │   │   ├── SRR1039516_2_fastqc.zip\n│   │   │   ├── SRR1039516_fastqc.html\n│   │   │   ├── SRR1039516_fastqc.zip\n│   │   │   ├── SRR1039517_1_fastqc.html\n│   │   │   ├── SRR1039517_1_fastqc.zip\n│   │   │   ├── SRR1039517_2_fastqc.html\n│   │   │   ├── SRR1039517_2_fastqc.zip\n│   │   │   ├── SRR1039520_1_fastqc.html\n│   │   │   ├── SRR1039520_1_fastqc.zip\n│   │   │   ├── SRR1039520_2_fastqc.html\n│   │   │   ├── SRR1039520_2_fastqc.zip\n│   │   │   ├── SRR1039520_fastqc.html\n│   │   │   ├── SRR1039520_fastqc.zip\n│   │   │   ├── SRR1039521_1_fastqc.html\n│   │   │   ├── SRR1039521_1_fastqc.zip\n│   │   │   ├── SRR1039521_2_fastqc.html\n│   │   │   ├── SRR1039521_2_fastqc.zip\n│   │   │   ├── SRR1039521_fastqc.html\n│   │   │   └── SRR1039521_fastqc.zip\n│   │   ├── hg38_genome\n│   │   │   └── GRCh38.primary_assembly.genome.fa\n│   │   └── hg38_index\n│   │       ├── hg38_index.00.b.array\n│   │       ├── hg38_index.00.b.tab\n│   │       ├── hg38_index.files\n│   │       ├── hg38_index.log\n│   │       ├── hg38_index.reads\n│   │       ├── subread-index-sam-186134-meVx1N\n│   │       └── subread-index-sam-193622-AYN0aX\n│   ├── rna_seq.Rmd\n│   └── rna_seq.html\n└── rnaseq_onecut\n    ├── README.txt\n    ├── Rmarkdown\n    │   ├── rnaseq_eind.Rmd\n    │   ├── rnaseq_eind.html\n    │   ├── rnaseq_eind.log\n    │   ├── rnaseq_eind.tex\n    │   ├── v1_Eindopdracht.Rmd\n    │   └── v2_Eindopdracht.Rmd\n    ├── code\n    ├── data\n    └── output\n        ├── 6_volcano_plot-1.png\n        ├── perbase_SRR7866699.png\n        └── perseq_SRR7866699.pngfs::dir_tree(here::here(\"\"))\n\nC:/Users/31642/Documents/Study/Vakken/Bioinformatica/DSBF2/dsbf2_workflows_portfolio/Portfolio\n+-- bibliography_portfolio\n|   +-- bibliography_portfolio.bib\n|   \\-- files\n|       +-- 11\n|       |   \\-- Sathyanesan and Gallo - 2019 - Cerebellar contribution to locomotor behavior A n.pdf\n|       +-- 13\n|       |   \\-- Dudel et al. - Monitoring trends and differences in COVID-19 case.pdf\n|       +-- 15\n|       |   \\-- Delgado-Gallegos et al. - 2021 - Teaching Anxiety, Stress and Resilience During the.pdf\n|       +-- 18\n|       |   \\-- www.noldus.com.html\n|       +-- 3\n|       |   \\-- Vinueza Veloz et al. - 2012 - The effect of an mGluR5 inhibitor on procedural me.pdf\n|       +-- 5\n|       |   \\-- Schonewille et al. - 2011 - Reevaluating the Role of LTD in Cerebellar Motor L.pdf\n|       +-- 7\n|       |   \\-- Vinueza Veloz et al. - 2015 - Cerebellar control of gait and interlimb coordinat.pdf\n|       \\-- 9\n|           \\-- Sathyanesan et al. - 2018 - Neonatal brain injury causes cerebellar learning d.pdf\n+-- code\n|   +-- 00_functions.R\n|   +-- 01_input.R\n|   +-- 02_analysis.R\n|   +-- 03_excess.R\n|   +-- Fig_1.R\n|   +-- guerrilla_tactics.R\n|   +-- pdftopng.R\n|   \\-- Tests.R\n+-- data\n|   +-- baseline_excess_pclm_5.csv\n|   +-- data_cv.R\n|   +-- dengue_data.csv\n|   +-- dengue_tidy.csv\n|   +-- dengue_tidy.rds\n|   +-- Dudel_et_al_CFR_Decomposition.xlsx\n|   +-- Excel_spreadsheets_decomposition.xlsx\n|   +-- flu_data.csv\n|   +-- flu_tidy.csv\n|   +-- flu_tidy.rds\n|   +-- gapdat_tidy.csv\n|   +-- gapdat_tidy.rds\n|   \\-- inputdata.csv\n+-- output\n|   +-- AppendixTab1.xlsx\n|   +-- AppendixTab2.xlsx\n|   +-- AppendixTab3.xlsx\n|   +-- AppendixTab4.xlsx\n|   +-- AppendixTab5.xlsx\n|   +-- AppendixTab6.xlsx\n|   +-- Fig_1.jpg\n|   +-- Table2.xlsx\n|   +-- Table3.xlsx\n|   \\-- thumbnail_IMG_7494.jpg\n+-- Portfolio.Rproj\n+-- README.txt\n\\-- Rmarkdowns\n    +-- awesome-cv.cls\n    +-- fonts\n    |   +-- FontAwesome.ttf\n    |   +-- Roboto-Bold.ttf\n    |   +-- Roboto-BoldItalic.ttf\n    |   +-- Roboto-Italic.ttf\n    |   +-- Roboto-Light.ttf\n    |   +-- Roboto-LightItalic.ttf\n    |   +-- Roboto-Medium.ttf\n    |   +-- Roboto-MediumItalic.ttf\n    |   +-- Roboto-Regular.ttf\n    |   +-- Roboto-Thin.ttf\n    |   \\-- Roboto-ThinItalic.ttf\n    +-- portfolio_opdracht1.html\n    +-- portfolio_opdracht1_1.html\n    +-- portfolio_opdracht1_1.Rmd\n    +-- portfolio_opdracht1_2.html\n    +-- portfolio_opdracht1_2.Rmd\n    +-- portfolio_opdracht2.html\n    +-- Portfolio_opdracht2.Rmd\n    +-- Portfolio_opdracht3.html\n    +-- Portfolio_opdracht3_1.pdf\n    +-- Portfolio_opdracht3_1.Rmd\n    +-- Portfolio_opdracht3_2.html\n    +-- Portfolio_opdracht3_2.Rmd\n    +-- Portfolio_opdracht5.html\n    +-- Portfolio_opdracht5.Rmd\n    +-- Portfolio_opdracht7.html\n    \\-- Portfolio_opdracht7.Rmd\nread_file(here::here(\"README.md\"))\n#> [1] \"# lunareclipse.github.io\\r\\n\\r\\nThis repository contains my portfolio. It is ment to showcase my skills in data analysis. My goal is to be able to present to a future employer what it is I can do and such this portfolio will change greatly over time. As of now I am very much in the beginning of my journey and looking for an internship to hown my skills in a certain direction. For me my interest lies especially in next generation sequencing. I have worked on some NGS projects trough my university and I very much like how it goes hand in hand with understanding the biology of organisms to better interpret the results. \\r\\nIn this project most of the coding is done in R, with a little bit of Bash, CSS, YML and HTML on the side.\""},{"path":"looking-ahead.html","id":"looking-ahead","chapter":"5 Looking ahead","heading":"5 Looking ahead","text":"demonstrate ability learn new skills continue develop try learn much can four days single cell next generation sequencing.chose skill NGS something find interesting NGS single cell sounds awesome.","code":""},{"path":"looking-ahead.html","id":"plan","chapter":"5 Looking ahead","heading":"5.0.1 Plan","text":"First small overview want structure 4 days make sure best make use time. end can hopefully run analysis, part one isn’t complicated. end need read search usable data used research try mimic done. Hopefully future allow beter develop skills needed run analysis Single Cell NGS data.","code":""},{"path":"looking-ahead.html","id":"day-1","chapter":"5 Looking ahead","heading":"5.0.2 Day 1","text":"","code":""},{"path":"projects.html","id":"projects","chapter":"6 Projects","heading":"6 Projects","text":"","code":""},{"path":"projects.html","id":"noldus-project","chapter":"6 Projects","heading":"6.1 Noldus project","text":"Noldus Inc. (“Noldus | Advance Behavioral Research” (n.d.)) University Applied sciences Utrecht (website HU) worked team 5 create shiny app help analyse data acquired one research products: Erasmus Ladder. Erasmus Ladder setup consists horizontal ladder determine differences behavior mice recording steps made selection sensors. Variations amount steps made type steps made mice can recorded accurately allows statistical analysis determine differences groups.","code":""},{"path":"projects.html","id":"the-application","chapter":"6 Projects","heading":"6.1.1 The application","text":"application analyses data acquired Erasmus Ladder plots exploratory publication ready graphs together statistical analysis researchers easily interpret results. One challenges creating application figuring kind data interesting researchers. reading articles using device speaking researchers field made selection publication ready graph functions read data formatted Erasmus ladder. also made selection exploratory graphs meant help researches get overview data showing basic information like amount mice used, different groups mice, etc. graphs portray overview things like different types steps mice made different groups.","code":""},{"path":"projects.html","id":"example-graphs","chapter":"6 Projects","heading":"6.1.2 Example graphs","text":"examples data interesting research purposes looked selection different articles. us important criteria articles use Erasmus ladder mostly interested interpreted data . type graphs used portrayed data important us base decisions graphs want application produce based input data. One first graphs found interesting provides clear visual variances steps made different groups mice (María Fernanda Vinueza Veloz et al. 2015). graph two different groups mice, one Purkinje cell deficiency (Pcd) one control group make runs crossing Erasmus ladder. step recorded type steps determined based distance traveled. page 3517 (Fig. 3) article can observe difference variation steps used might caused due Purkinje cell deficiency.\nVersions step type graphs plotted session can also found research (Sathyanesan Gallo 2019; Sathyanesan et al. 2018). graphs depict decrease variation steps used control groups time. might caused mice learning cross bridge efficiently. different type graph looked reaction mice different ques guide mice device. research (M. F. Vinueza Veloz et al. 2012) gave us fresh look data helped guide decisions data might seen relevant.also looked types cerebellum research (Schonewille et al. 2011) selected data better answer research questions. Although mostly applicable project give insight slight differences can .","code":""},{"path":"resume.html","id":"resume","chapter":"7 Resume","heading":"7 Resume","text":"likely already received C.V. case haven’t .","code":""},{"path":"relational-databases.html","id":"relational-databases","chapter":"8 Relational Databases","heading":"8 Relational Databases","text":"Working relational databases means many cases communicate using SQL. pull store data important follows small demonstration can done.first start loading required data short analysis. use data gapminder, data set dslabs package use dengue flu data sets.\nloading data need make tidy make easier work R. also make allot easier work compare data load DBeaver using SQL.finish plot simple graphs show bit data merged.","code":"\n############ Load in Data ##############################################\n## Gapminder data from the dslabs package\n  gapdat <- gapminder\n  gapdata_tidy <- gapminder\n## Dengue en flu data \n  dengue_data <- read_csv(here::here(\"data/dengue_data.csv\"), skip = 10)\n  flu_data <- read_csv(here::here(\"data/flu_data.csv\"), skip = 10)\n########################################################################\n############################## Tidy Data ################################\n\n## Gapminder data looks very tidy so no changes there except for the year column, that needs to become Date\n  gapdata_tidy$year <- as.character(gapdata_tidy$year)\n  colnames(gapdat) <- c(\"country\", \"Date\", \"infant_mortality\", \"life_expectancy\", \"fertility\", \"population\", \"gdp\", \"continent\", \"region\")\n\n## flu data tidy:\n  flu_data_tidy <- flu_data %>% pivot_longer(cols = -c(Date),\n                                             names_to = \"country\",\n                                             values_to = \"count_cases\") \n  ## dengue data tidy:\n  dengue_data_tidy <- dengue_data %>% pivot_longer(cols = -c(Date),\n                                             names_to = \"country\",\n                                             values_to = \"values\") \n########################### Export the Data ############################\n\n## Now we are going to write off the tidy data sets as csv and rds to export them later into the database\n  write_csv(gapdat_tidy, path = here::here(\"data/gapdat_tidy.csv\"))\n  write_csv(flu_data_tidy, path = here::here(\"data/flu_tidy.csv\"))\n  write_csv(dengue_data_tidy, path = here::here(\"data/dengue_tidy.csv\"))\n  \n  write_rds(gapdat_tidy, path = here::here(\"data/gapdat_tidy.rds\"))\n  write_rds(flu_data_tidy, path = here::here(\"data/flu_tidy.rds\"))\n  write_rds(dengue_data_tidy, path = here::here(\"data/dengue_tidy.rds\"))\n\n  ## In DBeaver we now make a new database to store the data\nCREATE DATABASE fludata;\n## Then we connect to the database\ncon <- dbConnect(RPostgres::Postgres(), \n                 dbname = \"fludata\", \n                 host=\"localhost\", \n                 port=\"5432\", \n                 user=\"postgres\", \n                 password=\"warLUNA112!\") \n\n## And create new tables with the data frames we made earlier\ndbWriteTable(con, \"flu_data\", flu_data_tidy)\ndbWriteTable(con, \"dengue_data\", dengue_data_tidy)\ndbWriteTable(con, \"gapminder_data\", gapdat_tidy)## Then in SQL we define the primary keys of the tables (Date and country)\nalter table public.flu_data \n    add constraint PK_flu_data primary key (Date, country);\n    \nalter table public.dengue_data \n    add constraint PK_dengue_data primary key (Date, country);\n\nalter table public.gapminer_data \n    add constraint PK_gapminder_data primary key (Date, country);\n\n## And we inspect the data from the table flu_data to check if all went well\nselect * from flu_data;\n######################## Inspect the Data ######################################\n## Now that the data has been stored in the database we can inspect it in R using the connection made\n\n# Shows the tables\ndbListTables(con)\n\n# Shows the table of gaminder_data\ndbGetQuery(con, 'SELECT * FROM gapminder_data')\n####################### Modify the gapminder data ##############################\n# Next we want to join the 3 data frames together based on the year and country columns\n## flu data tidy:\n  flu_data_tidy <- flu_data_tidy %>% separate(col = Date, into = c(\"year\", \"Month\", \"Day\"), sep = \"-\")\n## dengue data tidy:\n  dengue_data_tidy <- dengue_data_tidy  %>% separate(col = Date, into = c(\"year\", \"Month\", \"Day\"), sep =\"-\")\n    \nsic <- dengue_data_tidy %>% full_join(flu_data_tidy, by=c(\"year\",\"country\", \"Day\", \"Month\"))\n\nmerged_dat <- sic %>% full_join(gapdata_tidy, by=c(\"country\", \"year\"))"},{"path":"parameters.html","id":"parameters","chapter":"9 Parameters","heading":"9 Parameters","text":"page look use parameters Rmarkdown. end use data European Center Disease Control COVID-19 case data. goal create two separate graphs, one showing COVID-19 related cases specified month year country COVID-19 related deaths specified month year country. end make parameters customisable month, year country graphs supposed show.Parameters defined yaml header Rmarkdown documents can used just like variable code. One great advantage can customized fly change function output. case defined three parameters:can change date combinations want country long data input data code work. makes work -though wrote function code custom input function complete Rmarkdown file. handy indeed.page show code can take look parameters used . can find parameters used variables: params$parameter_nameParameters great work save time want change small things code already used . personally prefer writing good functions reusing parameters great tool apply aswell.","code":"\n###################### Load in the Data ########################################\n\n## First lets load in the Data kindly supplied by the ECDC. \n  data <- read_csv(here::here(\"data/covid_data.csv\"))\n\n######################## Data wrangling ########################################\n## Now we are going to change the date data type to date instead of chr.\n  data$dateRep <- as.Date(data$dateRep, \"%d/%m/%Y\")\n\n############################# Graphs ###########################################\n\n## Because the data is already tidy enough we don't need to wrangle further and can simply start plotting the graphs we want.\n  \n# Graph #1, the amount of covid-19 cases in a selected country for a selected period of time\n  data %>% select(month, year, day, cases, countriesAndTerritories, dateRep) %>% \n    filter(dateRep >= as.Date(params$date_from) & dateRep <= as.Date(params$date_until), countriesAndTerritories %in% c(params$country)) %>%\n    ggplot(aes(x=dateRep, y=cases)) + \n    geom_line(aes(color = countriesAndTerritories)) +\n    labs(title= paste(\"Covid-19 Cases in\", params$country),\n         subtitle = paste(params$date_from, params$date_until),\n         x= \"Date\",\n         y= \"Number of Cases\") +\n    scale_color_manual(values=c(\"turquoise3\")) +\n    theme_classic()\n\n# Graph #2, The amount of covid-19 Deaths in a selected country for a selected period of time. If I didn't work with parameters I would have build a function for these two graphs seeing as they are very similar in code. \n  data %>% select(month, year, day, deaths, countriesAndTerritories, dateRep) %>% \n    filter(dateRep >= as.Date(params$date_from) & dateRep <= as.Date(params$date_until), countriesAndTerritories %in% c(params$country)) %>%\n    ggplot(aes(x=dateRep, y=deaths)) + \n    geom_line(aes(color = countriesAndTerritories)) +\n    labs(title= paste(\"Covid-19 Deaths in\", params$country),\n         subtitle = paste(params$date_from, params$date_until),\n         x= \"Date\",\n         y= \"Number of Deaths\") +\n    scale_color_manual(values=\"turquoise4\") +\n    theme_classic()"},{"path":"normalizeme.html","id":"normalizeme","chapter":"10 normalizeme","heading":"10 normalizeme","text":"One skills love improve building usefull packages. don’t much experience enjoy progress making function wich usefull people. first function wrote actually still use min-max scaling normalisation function. end descided create package redefine function , knows maybe someone else might want use !goal normalizeme help normalization data. also contains functions help plot simple graph, case want visualize normalized data quickly insignificant data set just fun .important function package minmax().","code":""},{"path":"normalizeme.html","id":"installation","chapter":"10 normalizeme","heading":"10.1 Installation","text":"can install development version normalizeme GitHub :","code":"\n# install.packages(\"devtools\")\ndevtools::install_github(\"LunarEclipse112/normalizeme\")"},{"path":"references.html","id":"references","chapter":"11 References","heading":"11 References","text":"","code":""},{"path":"references.html","id":"pages","chapter":"11 References","heading":"11.1 404 pages","text":"default, users directed 404 page try access webpage found.","code":""},{"path":"references.html","id":"references-1","chapter":"11 References","heading":"11.2 References","text":"","code":""}]
